import streamlit as st
import time
import json
import sqlite3
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# é…ç½®ä¿¡æ¯
DATA_FILE = "./data/processed_data.json"
EMBEDDING_MODEL_NAME = r"G:\shujuwajue\4\exp04-easy-rag-system\exp04-easy-rag-system\all-MiniLM-L6-v2"
GENERATION_MODEL_NAME = r"G:\shujuwajue\4\exp04-easy-rag-system\exp04-easy-rag-system\Qwen2.5-0.5B"
TOP_K = 15  # è¦å±•ç¤ºçš„æ–‡æ¡£æ•°é‡
FETCH_K = 100  # ä»æ•°æ®åº“ä¸­æ£€ç´¢çš„æ–‡æ¡£æ€»æ•°
DB_PATH = './data/medical_rag.db'
MIN_ABSTRACT_LENGTH = 300  # æ‘˜è¦æœ€å°é•¿åº¦ï¼Œç”¨äºè¿‡æ»¤æ–‡æ¡£

# åˆå§‹åŒ– SQLite æ•°æ®åº“
def init_db():
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            
            title TEXT,
            abstract TEXT,
            embedding BLOB
        )
    ''')
    conn.commit()
    return conn

# åŠ è½½æ•°æ®
def load_data(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return []

# æ£€æŸ¥æ•°æ®åº“æ˜¯å¦ä¸ºç©º
def check_db_empty(conn):
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM documents")
    count = cursor.fetchone()[0]
    return count == 0  # å¦‚æœä¸ºç©ºï¼Œè¿”å› True

# åµŒå…¥æ¨¡å‹
@st.cache_resource
def load_embedding_model(model_name):
    return SentenceTransformer(model_name)

# ç”Ÿæˆæ¨¡å‹
@st.cache_resource
def load_generation_model(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return model, tokenizer

# æ’å…¥æ•°æ®å¹¶æ˜¾ç¤ºè¿›åº¦
def insert_data(conn, data, embeddings):
    cursor = conn.cursor()
    progress_bar = st.progress(0)  # åˆå§‹åŒ–è¿›åº¦æ¡
    for i, (entry, embedding) in enumerate(zip(data, embeddings)):
        cursor.execute('''
            INSERT OR REPLACE INTO documents (id, title, abstract, embedding) VALUES (?, ?, ?, ?)
        ''', (entry['id'], entry['title'], entry['abstract'], embedding.tobytes()))
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.progress((i + 1) / len(data))
    conn.commit()
    progress_bar.empty()  # å®Œæˆåæ¸…é™¤è¿›åº¦æ¡

# åµŒå…¥æŸ¥è¯¢æ–‡æ¡£å¹¶è®¡ç®—ç›¸ä¼¼åº¦
def search_similar_documents(conn, query_embedding, top_k=TOP_K, fetch_k=FETCH_K):
    cursor = conn.cursor()
    cursor.execute("SELECT id, title, abstract, embedding FROM documents")
    documents = cursor.fetchall()

    # è®¡ç®—æ¯ä¸ªæ–‡æ¡£åµŒå…¥ä¸æŸ¥è¯¢åµŒå…¥çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    scores = []
    query_embedding_normalized = query_embedding / np.linalg.norm(query_embedding)  # æ­£è§„åŒ–æŸ¥è¯¢åµŒå…¥
    for doc in documents:
        # åªè€ƒè™‘æ‘˜è¦é•¿åº¦å¤§äºç­‰äº MIN_ABSTRACT_LENGTH çš„æ–‡æ¡£
        if len(doc[2]) < MIN_ABSTRACT_LENGTH:
            continue
        
        doc_embedding = np.frombuffer(doc[3], dtype=np.float32)  # å°† BLOB è¿˜åŸä¸ºæ•°ç»„
        if np.linalg.norm(doc_embedding) == 0:  # å¼‚å¸¸æƒ…å†µæ£€æŸ¥
            continue
        doc_embedding_normalized = doc_embedding / np.linalg.norm(doc_embedding)  # æ­£è§„åŒ–æ–‡æ¡£åµŒå…¥
        score = np.dot(doc_embedding_normalized, query_embedding_normalized)  # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        scores.append((score, doc))

    # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é€‰å–å‰ fetch_k ä¸ªæ–‡æ¡£
    scores.sort(reverse=True, key=lambda x: x[0])
    return [(score, doc) for score, doc in scores[:fetch_k]]  # è¿”å›ç›¸ä¼¼åº¦å’Œæ–‡æ¡£å†…å®¹

# Streamlit ç•Œé¢è®¾å®š
st.set_page_config(layout="wide")
st.title("ğŸ“„ åŒ»ç–— RAG ç³»ç»Ÿ (SQLite)")
st.markdown(f"ä½¿ç”¨ `{EMBEDDING_MODEL_NAME}` å’Œ `{GENERATION_MODEL_NAME}`ã€‚")

# åˆå§‹åŒ–å’ŒåŠ è½½æ¨¡å‹
embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
generation_model, tokenizer = load_generation_model(GENERATION_MODEL_NAME)

# åˆå§‹åŒ–æ•°æ®åº“
conn = init_db()

# æ£€æŸ¥æ•°æ®åº“æ˜¯å¦ä¸ºç©ºå¹¶åŠ è½½æ•°æ®
if check_db_empty(conn):
    pubmed_data = load_data(DATA_FILE)
    if pubmed_data:
        # ç”ŸæˆåµŒå…¥
        embeddings = embedding_model.encode([doc['abstract'] for doc in pubmed_data], show_progress_bar=True)
        insert_data(conn, pubmed_data, embeddings)
    else:
        st.warning("åŠ è½½çš„æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ’å…¥åˆ°æ•°æ®åº“ã€‚")

# RAG äº¤äº’éƒ¨åˆ†
query = st.text_input("è¯·æå‡ºå…³äºå·²ç´¢å¼•åŒ»ç–—æ–‡ç« çš„é—®é¢˜:", key="query_input")

if st.button("è·å–ç­”æ¡ˆ", key="submit_button") and query:
    start_time = time.time()

    # å¯¹æŸ¥è¯¢è¿›è¡ŒåµŒå…¥
    query_embedding = embedding_model.encode(query)

    # æ£€ç´¢ä¸æŸ¥è¯¢ç›¸ä¼¼çš„æ–‡æ¡£
    retrieved_docs_with_scores = search_similar_documents(conn, query_embedding, top_k=TOP_K)

    if not retrieved_docs_with_scores:
        st.warning("åœ¨æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
    else:
        st.subheader("æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
        for score, doc in retrieved_docs_with_scores[:TOP_K]:  # åªæ˜¾ç¤ºå‰TOP_Kç¯‡
            with st.expander(doc[1], expanded=False):  # å±•å¼€æ ‡é¢˜ä»¥æŸ¥çœ‹å…¨æ–‡å†…å®¹
                st.write(f"**æ ‡é¢˜:** {doc[1]}")  # æ˜¾ç¤ºæ ‡é¢˜
                st.write(f"**å†…å®¹:** {doc[2]}")  # æ˜¾ç¤ºæ‘˜è¦
                st.write(f"**ç›¸ä¼¼åº¦å¾—åˆ†:** {score:.4f}")  # æ˜¾ç¤ºç›¸ä¼¼åº¦å¾—åˆ†
        
        # ç”Ÿæˆç­”æ¡ˆå¹¶æ˜¾ç¤ºè¿›åº¦
        st.subheader("ç”Ÿæˆçš„ç­”æ¡ˆ:")
        context = "\n\n---\n\n".join([f"æ ‡é¢˜: {doc[1]}\nå†…å®¹: {doc[2]}" for _, doc in retrieved_docs_with_scores[:TOP_K]])
        prompt = f"""æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆï¼š
        ä¸Šä¸‹æ–‡ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜: {query}

        ç­”æ¡ˆï¼š
        """
        
        inputs = tokenizer(prompt, return_tensors="pt").to(generation_model.device)

        # æ·»åŠ è¿›åº¦æ¡è¿›è¡Œç”Ÿæˆ
        completion_progress_bar = st.progress(0)  # åˆå§‹åŒ–è¿›åº¦æ¡
        with torch.no_grad():
            outputs = generation_model.generate(
                **inputs,
                max_new_tokens=150,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=True
            )
        
            # åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ›´æ–°è¿›åº¦æ¡
            for i in range(101):
                completion_progress_bar.progress(i)
                time.sleep(0.02)  # æ¨¡æ‹Ÿç”Ÿæˆæ—¶é—´

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        st.write(response)
        completion_progress_bar.empty()  # å®Œæˆåæ¸…é™¤è¿›åº¦æ¡

    end_time = time.time()
    st.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

# å…³é—­æ•°æ®åº“è¿æ¥
conn.close()
